{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# runtime-sandbox\n",
    "1.26.23\n",
    "\n",
    "Taking a crack at this. \n",
    "Figured out how to get runtime and store as a variable. \n",
    "I think it makes the most sense to represent dataset size in terms of number of observations in the training set. Mb gets a bit screwy when you consider log vs linear peptide quants datasets: the same datset will require more Mbs to encode as linear vs log. And you definetly want to look at training set # of obs, as this is what the model actually imputes. And because the partition can be stochastic, and can potentially alter the number of observations by quite a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from sklearn.impute import KNNImputer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# suppressing this CUDA initialization warning I always get\n",
    "    # this could be dangerous\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# import my modules\n",
    "sys.path.append('../../../../bin/')\n",
    "from models.linear import GradNMFImputer\n",
    "import util_functions\n",
    "import intermediate_plots\n",
    "\n",
    "# for missForest:\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import numpy2ri, r\n",
    "\n",
    "# plotting templates\n",
    "sns.set(context=\"talk\", style=\"ticks\") \n",
    "pal = sns.color_palette()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioning params\n",
    "val_frac = 0.3\n",
    "test_frac = 0.0\n",
    "# setting this to 0 ensures that no peptides will be filtered out\n",
    "min_present = 0     # during partitioning\n",
    "q_anchor=0.3  # these three for MNAR partition \n",
    "t_std=0.35\n",
    "brnl_prob=0.4\n",
    "\n",
    "# NMF model params\n",
    "n_factors = 4                 # 4 is default\n",
    "tolerance = 0.0001            # 0.0001 is default\n",
    "max_epochs = 1000             # 1000 is default\n",
    "learning_rate = 0.01          # 0.01 is default\n",
    "batch_size = 64               # 64 is default\n",
    "loss_func = \"MSE\"\n",
    "\n",
    "# kNN params\n",
    "k_neighbors = 4\n",
    "\n",
    "# missForest impute params\n",
    "n_trees = 100                 # 100 is default, according to mf manual\n",
    "max_iters_mf = 10             # 10 is default, according to mf manual\n",
    "r_seed = 36                   # the random seed for rpy2\n",
    "\n",
    "# the random number generator\n",
    "rng = np.random.default_rng(seed=18)\n",
    "\n",
    "# the random state for the partition\n",
    "split_rand_state = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in peptide quants dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = \"/net/noble/vol2/home/lincolnh/code/2021_ljharris_ms-impute/data/peptides-data/\"\n",
    "df = \"PXD006109_peptides.csv\"\n",
    "pxd = \"PXD006109\"\n",
    "\n",
    "quants_raw = pd.read_csv(full_path + df)\n",
    "quants_raw[quants_raw == 0] = np.nan\n",
    "\n",
    "quants = np.array(quants_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCAR partition \n",
    "# train, val, test = util_functions.split(\n",
    "#                                     quants, \n",
    "#                                     val_frac=val_frac,\n",
    "#                                     test_frac=test_frac, \n",
    "#                                     min_present=min_present,\n",
    "#                                     random_state=split_rand_state,\n",
    "# )\n",
    "# MNAR partition \n",
    "train, val = util_functions.MNAR_partition_thresholds_matrix(\n",
    "                                    quants, \n",
    "                                    q_anchor=q_anchor, \n",
    "                                    t_std=t_std, \n",
    "                                    brnl_prob=brnl_prob, \n",
    "                                    min_pres=min_present,\n",
    "                                    rand_state=split_rand_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the missingness fractions of each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv frac original:  0.165\n",
      "mv frac train:  0.382\n",
      "mv frac validation:  0.783\n"
     ]
    }
   ],
   "source": [
    "orig_mv_frac = np.count_nonzero(np.isnan(quants)) / quants.size\n",
    "train_mv_frac = np.count_nonzero(np.isnan(train)) / train.size\n",
    "val_mv_frac = np.count_nonzero(np.isnan(val)) / val.size\n",
    "\n",
    "print(\"mv frac original: \", np.around(orig_mv_frac, decimals=3))\n",
    "print(\"mv frac train: \", np.around(train_mv_frac, decimals=3))\n",
    "print(\"mv frac validation: \", np.around(val_mv_frac, decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the optimal number of batches for training (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595\n"
     ]
    }
   ],
   "source": [
    "if len(~np.isnan(train)) > 100:\n",
    "    n_batches = int(np.floor(len(~np.isnan(train)) / batch_size))\n",
    "    # setting the minimum n_batches to 100\n",
    "    n_batches = max(n_batches, 100) \n",
    "else: \n",
    "    n_batches = 1\n",
    "\n",
    "print(n_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many observations are in the training set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471302\n"
     ]
    }
   ],
   "source": [
    "n_obs = np.count_nonzero(~np.isnan(train))\n",
    "print(n_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time various imputation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–‹         | 64/1000 [01:48<26:31,  1.70s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopping triggered: standard criteria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nmf runtime (sec):  109.57062029838562\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "# get the start time, measured since the Unix epoch\n",
    "nmf_start_sec = time.time()\n",
    "\n",
    "# init model \n",
    "nmf_model = GradNMFImputer(\n",
    "                n_rows = train.shape[0], \n",
    "                n_cols = train.shape[1], \n",
    "                n_factors=n_factors, \n",
    "                stopping_tol=tolerance,\n",
    "                train_batch_size=n_batches, \n",
    "                eval_batch_size=n_batches,\n",
    "                n_epochs=max_epochs, \n",
    "                loss_func=loss_func,\n",
    "                optimizer=torch.optim.Adam,\n",
    "                optimizer_kwargs={\"lr\": learning_rate},\n",
    "                non_negative=True,\n",
    "                rand_seed=rng.random(),\n",
    ")\n",
    "# fit and transform\n",
    "nmf_recon = nmf_model.fit_transform(train, val)\n",
    "\n",
    "# get the elapsed time\n",
    "nmf_end_sec = time.time()\n",
    "nmf_sec_elapsed = nmf_end_sec - nmf_start_sec\n",
    "\n",
    "print(\"nmf runtime (sec): \", nmf_sec_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "\n",
    "# # get the start time, measured since the Unix epoch\n",
    "# knn_start_sec = time.time()\n",
    "\n",
    "# knn_model = KNNImputer(n_neighbors=k_neighbors)\n",
    "# knn_recon = knn_model.fit_transform(train)\n",
    "\n",
    "# # get the elapsed time\n",
    "# knn_end_sec = time.time()\n",
    "# knn_sec_elapsed = knn_end_sec - knn_start_sec\n",
    "\n",
    "# print(\"knn runtime (sec): \", knn_sec_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample min impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min runtime (sec):  0.02395343780517578\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# get the start time, measured since the Unix epoch\n",
    "min_start_sec = time.time()\n",
    "\n",
    "col_min = np.nanmin(train, axis=0)\n",
    "nan_idx = np.where(np.isnan(train))\n",
    "min_recon = train.copy()\n",
    "# nan_idx[1] -> take index of column\n",
    "min_recon[nan_idx] = np.take(col_min, nan_idx[1])\n",
    "\n",
    "# get the elapsed time\n",
    "min_end_sec = time.time()\n",
    "min_sec_elapsed = min_end_sec - min_start_sec\n",
    "\n",
    "print(\"min runtime (sec): \", min_sec_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian random sample impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std runtime (sec):  0.05980038642883301\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# get the start time, measured since the Unix epoch\n",
    "std_start_sec = time.time()\n",
    "\n",
    "# get the column mins\n",
    "col_min = np.nanmin(train, axis=0)\n",
    "\n",
    "# get the mean and std of the entire training matrix\n",
    "train_mean = np.nanmean(train)\n",
    "train_sd = np.nanstd(train)\n",
    "\n",
    "# get the indicies of the MVs \n",
    "nan_idx = np.where(np.isnan(train))\n",
    "std_recon = train.copy()\n",
    "\n",
    "# how many total MVs? \n",
    "n_mv = len(nan_idx[0])\n",
    "\n",
    "# fill in the MVs with random draws \n",
    "std_recon[nan_idx] = rng.normal(\n",
    "                            loc=np.mean(col_min), \n",
    "                            scale=np.std(col_min), \n",
    "                            size=n_mv\n",
    ")\n",
    "\n",
    "# don't want negative values\n",
    "std_recon = np.abs(std_recon)\n",
    "\n",
    "# get the elapsed time\n",
    "std_end_sec = time.time()\n",
    "std_sec_elapsed = std_end_sec - std_start_sec\n",
    "\n",
    "print(\"std runtime (sec): \", std_sec_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### missForest impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# get the start time, measured since the Unix epoch\n",
    "# mf_start_sec = time.time()\n",
    "\n",
    "# set_seed = r('set.seed')\n",
    "# set_seed(r_seed)\n",
    "\n",
    "# base = importr(\"base\")\n",
    "# doParallel = importr(\"doParallel\")\n",
    "# rngtools = importr(\"rngtools\")\n",
    "# missForest = importr(\"missForest\")\n",
    "\n",
    "# # activate automatic conversion of NumPy arrays\n",
    "# numpy2ri.activate()\n",
    "\n",
    "# # set up parallelization\n",
    "#     # not totally sure how to set this\n",
    "# doParallel.registerDoParallel(cores=12)\n",
    "\n",
    "# # run missForest\n",
    "# mf_recon, err = missForest.missForest(\n",
    "#                             train, \n",
    "#                             maxiter=max_iters_mf,\n",
    "#                             ntree=n_trees,\n",
    "#                             parallelize=\"forests\", \n",
    "#                             verbose=True,\n",
    "# )\n",
    "# mf_recon = np.array(mf_recon)\n",
    "\n",
    "# get the elapsed time\n",
    "# mf_end_sec = time.time()\n",
    "# mf_sec_elapsed = mf_end_sec - mf_start_sec\n",
    "\n",
    "# print(\"mf runtime (sec): \", mf_sec_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record in a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>n observations</th>\n",
       "      <th>NMF sec</th>\n",
       "      <th>kNN sec</th>\n",
       "      <th>min sec</th>\n",
       "      <th>std sec</th>\n",
       "      <th>mf sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PXD006109</td>\n",
       "      <td>471302</td>\n",
       "      <td>109.57062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023953</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset n observations    NMF sec  kNN sec   min sec  std sec  mf sec\n",
       "0  PXD006109         471302  109.57062      0.0  0.023953   0.0598     0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"dataset\", \n",
    "        \"n observations\", \n",
    "        \"NMF sec\", \n",
    "        \"kNN sec\", \n",
    "        \"min sec\", \n",
    "        \"std sec\", \n",
    "        \"mf sec\"]\n",
    ")\n",
    "\n",
    "toadd = {\n",
    "    \"dataset\" : pxd, \n",
    "    \"n observations\" : n_obs,\n",
    "    \"NMF sec\" : nmf_sec_elapsed,\n",
    "    \"kNN sec\" : 0.0,\n",
    "    \"min sec\" : min_sec_elapsed,\n",
    "    \"std sec\" : std_sec_elapsed,\n",
    "    \"mf sec\" : 0.0,\n",
    "}\n",
    "\n",
    "rt_df = rt_df.append(toadd, ignore_index=True)\n",
    "rt_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-impute-trim",
   "language": "python",
   "name": "ms-impute-trim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
