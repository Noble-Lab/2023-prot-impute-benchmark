\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage[margin=1in]{geometry}
\usepackage[parfill]{parskip}
\usepackage{authblk} % Better formatting of affiliations.
\usepackage{graphicx} % Allow graphics
\usepackage{xcolor} % Allow colored text.
\usepackage[sort&compress]{natbib} % Better bibliography formatting

% for nice url formatting, including auto linebreaks
\usepackage{xurl}

% makes nice looking tables
\usepackage{tabularx}

% get periods in figure captions instead of colons
\usepackage[labelsep=period]{caption}

% indicate FIXMEs with red text
\newcommand{\fixme}[1]{{\color{red}{#1}}}

\title{Evaluating proteomics imputation methods with improved criteria}

\author[1]{Lincoln Harris}
\author[2]{William E.\ Fondrie}
\author[3]{Sewoong Oh}
\author[1,3]{William S.\ Noble}

\affil[1]{Department of Genome Sciences, University of Washington}
\affil[2]{Talus Biosciences}
\affil[3]{Paul G.\ Allen School of Computer Science and Engineering,
  University of Washington}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Quantitative measurements produced by tandem mass spectrometry proteomics experiments typically contain a large proportion of missing values. This missingness hinders reproducibility, reduces statistical power, and makes it difficult to compare across samples or experiments. Although many methods exist for imputing missing values in proteomics data, in practice, the most commonly used methods are among the worst performing. Furthermore, previous benchmarking studies have focused on relatively simple measurements of error, such as the mean-squared error between the imputed and the held-out observed values.  Here we evaluate the performance of a set of commonly used imputation methods using three  practical, ``downstream-centric'' criteria, which measure the ability of imputation methods to reconstruct differentially expressed peptides, identify new quantitative peptides, and improve peptide lower limit of quantification. Our evaluation spans several experiment types and acquisition strategies, including data-dependent and data-independent acquisition. We find that imputation does not necessarily improve the ability to identify differentially expressed peptides, but that it can identify new quantitative peptides and improve peptide lower limit of quantification. We find that MissForest is generally the best performing method per our downstream-centric criteria. We also argue that exisiting imputation methods do not properly account for the variance of peptide quantifications and highlight the need for methods that do.
\end{abstract}

\section{Introduction}

The quantitative accuracy and sensitivity of tandem mass spectrometry proteomics experiments has increased dramatically in the past decade. In spite of this trend, proteomics experiments are still limited by excessive ``missingness,'' which refers to peptides that are present in the sample matrix but are not detected by the instrument. Missingness can be attributed to a variety of technical factors including ion suppression, co-eluting peptides, the lower limit of quantification of the instrument, and the failure to confidently assign peptides to all observed spectra \cite{Bramer:review, Webb-Robertson:review}. Although low abundance peptides are generally more likely to be missing, peptides may be missing across the entire range of intensities. Missingness decreases the statistical power of proteomics experiments, hinders reproducibility, and makes it difficult to compare across batches or experiments \cite{Bramer:review, Webb-Robertson:review}.

Imputation is a bioinformatic solution to the missingness problem. Imputation entails using statistical or machine learning procedures to estimate missing values in a data set. While still not widely accepted in the proteomics community, imputation has been standard practice for decades for analysis of gene expression \cite{knn-impute} and clinical and epidemiological data \cite{multi-impute-clinical}, and more recently astronomy \cite{astro-impute1, astro-impute2} and single-cell transcriptomic data \cite{ALRA, magic-scRNA}. Imputation methods for proteomics data (Table~\ref{tab:method-descrip}) fall into three broad categories: ``single-value replacement'' methods, in which all missing values are filled in with a single replacement value; ``local similarity'' methods, which use statistical models to learn patterns of local similarity in the data, for example between subsets of similar peptides or runs; and ``global similarity'' methods, which learn broad patterns of similarity across all peptides and runs.

\begin{table}
  \scriptsize
  \centering
  \begin{tabular}{lcp{3in}p{1in}}
    \hline
    Method & Type & Description & Examples \\
    \hline
    Zero replacement & S & Replace missing values with zeros & \\
    Mean replacement & S & Replace missing values with the mean peptide intensity for a peptide or sample & \\
    Low value replacement & S & Replace missing values with the lowest observed intensity in any sample (sample minimum) or peptide (peptide minimum) & \\
    Gaussian random sample & S & Randomly sample from a Gaussian distribution centered around the lowest observed intensity & Perseus \cite{Perseus} \\
    Regression & L & Linear regression is used to estimate missing values & lm, glm \\
   kNN & L & Weighted average intensity of $k$ most similar peptides & impute.knn \cite{knn-impute}, VIM \cite{VIM} \\
    MissForest & G & Nonparametric method to impute missing values
using a random forest classifier trained on the observed parts of the data set,
repeated until convergence & MissForest \cite{missForest} \\
    PCA & G & Run principal component analysis, impute missing values with the regularized reconstruction formulas and repeat until convergence & pcaMethods \cite{pcaMethods}, missMDA \cite{missMDA} \\
     \hline
  \end{tabular}
  \caption{{\bf Existing proteomics imputation methods.} Descriptions of general categories of imputation strategies and examples of specific tools that implement each strategy.  The ``Type'' column indicates whether the method uses single-value replacement (S), local similarity (L), or global similarity (G).
    \label{tab:method-descrip}}
\end{table}

It is not always clear what imputation method is best for a given proteomics data set. A number of studies benchmark imputation methods and offer guidelines for selecting an appropriate method \cite{Bramer:review, Webb-Robertson:review, DIMA, lazar, valikangas, dabke}. A general recommendation is that single-value replacement strategies rarely work well. Another is that the optimal imputation method depends on the structure of missingness in the data. Mass spectrometry-based proteomics experiments exhibit two major forms of missingness: missing completely at random (MCAR) and missing not at random (MNAR). MCAR describes the case in which missingness does not depend on any observed variable, that is, missingness occurs independent of peptide intensity or relationships between samples. In the case of MNAR, missingness \textit{is} dependent on some observed variable. For example, in mass spectrometry-based proteomics, missingness is often a function of peptide intensity, with more missingness occurring in peptides closer to the instrument's lower limit of quantification (LLOQ).

Most proteomics imputation method benchmarking studies use relatively simple performance measures that are not necessarily relevant to proteomics researchers. One common example is computing the mean squared error (MSE) between imputed and ground truth peptides quantifications for a withheld set of matrix entries. We argue that such evaluation metrics, while certainly valid, are neither the most relevant nor the most useful to proteomics researchers. We therefore introduce alternative, ``downstream-centric,'' criteria focused on differential expression, peptide LLOQ, and the total number of quantitative peptides in an experiment. We argue that these downstream-centric criteria are more relevant to the questions proteomics researchers typically seek to answer. Furthermore, we observe that the best-performing imputation methods per traditional criteria often differ from the best performing methods per our downstream-centric criteria. 

\begin{figure}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{figures/imp-lit-seq-results-update1.png}
  \end{center}
  \caption{{\bf Citation counts for the most commonly used proteomics imputation methods.} Results of a literature survey of \textit{Journal of Proteome Research} articles from January, 2019 -- January, 2023 are shown. Methods labeled ``other'' appear in just a single publication, and refer to the imp4p and QRLIC R packages, as well as methods based on Euclidean distances and randomly drawing from the entire peptide intensity range.  The full results of this literature search, including the names and DOIs of the identified studies, are included as Data 1. }
  \label{fig:citation-counts}
\end{figure} 

To decide which imputation methods to include in our study, we carried out a systematic literature review.  All \textit{Journal of Proteome Research} articles published between January 1, 2019, and January 31, 2023, were searched for the following terms: ``impute,'' ``imputed,'' ``imputation.'' For this survey we excluded methodological and benchmarking studies.  On the basis of the resulting citation counts (Figure~\ref{fig:citation-counts}), we selected four of the most popular imputation methods: $k$-nearest neighbor (kNN)  \cite{knn-impute}, MissForest \cite{missForest}, Gaussian sampling \cite{Perseus}, and low value replacement (Figure \ref{fig:citation-counts}).   We also include a non-negative matrix factorization (NMF) imputation method, which has recently been proposed for proteomics \cite{nmf-metabolomics, ms-impute, deep-impute}.  By focusing only on the most commonly used imputation methods, our aim is to provide a practical comparison that will be beneficial to experimental proteomicists. For this reason, seldom used R packages (e.g., imp4p, impSeqRob, QRLIC) have been omitted from our analysis. We also omit PCA-based methods, as we did not find a single recent study that used them. Additionally, we choose to conduct our analysis exclusively on peptide-level quantifications. Our reason for this is twofold: (i) summarizing peptide quantifications at the protein level reduces often-critical data heterogeneity \cite{humpty-dumpty}, and (ii) imputation generally performs better at the peptide level \cite{lazar}.

We evaluate the performance of each of the five selected methods with both traditional and downstream-centric criteria. The latter include the ability of an imputation method to: (i) reconstruct differentially expressed peptides, (ii) identify new quantitative peptides, and (iii) improve the LLOQ of peptides in a proteomics experiment. Our benchmarking study consists of a variety of data set types, including a serial dilution experiment, data-dependent and data-independent acquisition (DDA and DIA) studies, as well as label-free and isobaric labeled experiments. Critically, we include an unimputed condition for all three downstream-centric evaluation experiments, for evaluating whether imputation should be performed in the first place. Our findings suggest that imputation may not significantly improve the ability to detect differentially expressed peptides, but that it can identify new quantitative peptides and improve peptide LLOQ.

We also demonstrate that peptide quantifications exhibit greater than expected variance in relation to measured intensity. Ion detection is a Poisson process, and so quantifications from ion-counting mass spectrometers are often assumed to be Poisson distributed \cite{ms-dist-derivation, stat-theory-lcms}. We demonstrate that peptide quantification measurements are overdispersed and are thus not Poisson distributed, and that log transformation does not completely correct for this overdispersion. This finding is important because many existing imputation methods make distributional assumptions that are not actually met by mass spectrometry-based proteomics data. To our knowledge, no existing method models quantitative proteomics data with the proper distributional assumptions. This suggests the need for methods that employ variance stabilization prior to imputation, similar to strategies taken in genomics \cite{variance-stable, ZINB, neg-binom-scRNAseq}.

\section{Methods}

\subsection{Data sets}

For this study, we used 12 public quantitative proteomics data sets (Table \ref{tab:data-description}). Nine of the 12 data sets were accessed via the Proteomics Identification Database (PRIDE, \url{https://www.ebi.ac.uk/pride/}) \cite{PRIDE}, and are indicated with their ProteomeXchange (PXD) labels \cite{ProteomeXchange}. The remaining two data sets were obtained from the National Cancer Institute's Clinical Proteomic Tumor Analysis Consortium (CPTAC) data portal (\url{https://proteomic.datacommons.cancer.gov/pdc/}) \cite{CPTAC}. A complete list of the files obtained for each experiment is provided in Data 2. 

For experiments processed with MaxQuant, we used the provided ``peptides.txt'' files to generate peptide-by-run intensity matrices by selecting only the ``Sequence'' and ``Intensity'' columns. For experiments not processed with MaxQuant, we obtained peptide-spectrum match files and converted them to matrix format with custom scripts (available at \url{https://github.com/Noble-Lab/2023-prot-impute-benchmark}). The peptide quantification matrices from the two CPTAC studies were large (S047: 110k peptides $\times$ 226 samples; S051: 291k peptides $\times$ 35 samples). For efficiency, we downsampled both matrices by randomly selecting 40,000 peptides and 30 runs from each.

\begin{table}
  \centering
  \normalsize
  \begin{tabular}{lrrrrrr}
    \hline
    Identifier & Peptides & Runs & \% Missing & Quantification Software & Experiment Type & Citation \\
    \hline
    PXD016079 & 32999 & 31 & 45 & MaxQuant, MBR & DDA, LFQ & \cite{pxd016079} \\
    PXD006109 & 38124 & 20 & 17 & MaxQuant, MBR & DDA (BoxCar) & \cite{pxd006109} \\
    PXD014525 & 17208 & 36 & 92 & MaxQuant & DDA, LFQ & \cite{pxd014525} \\
    PXD034525 & 40346 & 10 & 13 & EncyclopeDIA, Skyline & DIA & \cite{smtg-maccoss} \\
    PXD014815 & 24204 & 42 & 29 & EncyclopeDIA, Skyline & DIA & \cite{matrix-matched-calib} \\
    PXD013792 & 2224 & 12 & 72 & MaxQuant & DDA, LFQ & \cite{pxd013792} \\
    PXD014156 & 697 & 20 & 55 & MaxQuant & DDA, LFQ & \cite{pxd014156} \\
    PXD006348 & 10362 & 24 & 72 & MaxQuant & DDA, LFQ & \cite{pxd006348} \\
    PXD011961 & 23415 & 23 & 46 & MaxQuant, MBR & DDA, LFQ & \cite{pxd011961} \\
    CPTAC-S047 & 40000 & 30 & 54 & Philosopher & DDA, TMT & \cite{CPTAC-S047} \\
    CPTAC-S051 & 40000 & 30 & 41 & Spectrum Mill & DDA, TMT & \cite{CPTAC-S051} \\
    PXD007683 & 38921 & 11 & 0 & Custom pipeline & DDA, TMT & \cite{pxd007683} \\
    \hline
  \end{tabular}
  \caption{{\bf Data set characteristics.} Description of the public proteomics data sets used in this study. The two CPTAC data sets were downsampled by randomly selecting 40,000 peptides and 30 runs each. ``MBR'' stands for ``match between runs,'' ``LFQ'' for ``label-free quantification,'' ``SPS'' for ``synchronous precursor selection'', and ``TMT'' for ``tandem mass tag.'' Quantification software references: MaxQuant \cite{MaxQuant}, EncyclopeDIA \cite{chromatogram-DIA}, Skyline \cite{skyline}, Philosopher \cite{philosopher}.
    \label{tab:data-description}}
\end{table}

\subsection{Traditional evaluation measures}

We first used a traditional train/test setup to evaluate the performance of imputation methods. In this approach, the values in the matrix are randomly segregated into two groups: a training set and a test set.  The imputation method is given the training set values, and we measure how well the method imputes the values in the test set.  For each data set, peptides with fewer than four present values in the training set were removed prior to splitting.  These tests were carried out using the following seven data sets: PXD014156, PXD011961, PXD014525, CPTAC-S051, CPTAC-S047, PXD034525, PXD014815 (Table \ref{tab:data-description}).

For each data set, we simulated missingness according to two procedures: MCAR and MNAR. For MCAR, 25\% of the present (i.e., non-missing) matrix entries were randomly selected for the test set. The remaining matrix entries were used as the training set.  For MNAR, we took a similar approach to the one described by Lazar \textit{et al.} \cite{lazar}. For a given peptide quantifications matrix, we constructed an equally sized \textit{thresholds matrix} filled with values sampled from a Gaussian distribution centered about the 30\textsuperscript{th} percentile of the distribution of quantifications, with standard deviation 0.6. For each element $X_{ij}$ in the peptides matrix, if the corresponding thresholds matrix element $T_{ij} < X_{ij}$, then $X_{ij}$ was assigned to the training set. Otherwise, a single Bernoulli trial with probability of success 0.75 was conducted.  If this Bernoulli trial was successful, then $X_{ij}$ was assigned to the test set. Otherwise $X_{ij}$ was assigned to the training set. Bernoulli success probability and Gaussian distribution mean and standard deviation were selected in such a way that 25\% of present matrix entries were ultimately assigned to the test set. The remaining 75\% were assigned to the training set. The distributions of the training and test set values following the MCAR and MNAR partitions are shown in Supplementary Figure \ref{fig:partitions}, for experiment PXD034525.

Once missing values had been simulated into the six matrices, imputation was performed with five procedures: NMF, kNN, MissForest, low value replacement (sample minimum) and Gaussian sample impute. A custom PyTorch model was used for NMF imputation. This model used an MSE loss function and stochastic gradient descent to converge on an ideal matrix factorization. This model is available at \url{https://github.com/Noble-Lab/ms_imputer}. For kNN, we used the KNNImputer implementation from scikit-learn. MissForest version 1.5 was used (\url{https://CRAN.R-project.org/package=missForest}) \cite{missForest}. Custom code was used for the low value replacement and Gaussian sample impute procedures. For Gaussian sample impute we attempted to replicate the procedure taken by Perseus \cite{Perseus}. For low value replacement, we filled in missing values with the lowest measured peptide intensity for each sample. NMF and kNN were performed with four latent factors and neighbors, respectively. MissForest was performed with 100 trees, the default setting.

Following imputation, we computed the MSE between observed and imputed values for each test set.

\subsection{Downstream-centric evaluation measures}

\subsubsection{Differential expression}

For differential expression analysis we obtained data from PXD034525, a DIA study of Alzheimer's disease \cite{smtg-maccoss}. Clinical samples had previously been assigned to experimental groups based on several genetic, histopathological and cognitive criteria. We compared differentially expressed peptides between (i) autosomal dominant Alzheimer's disease dementia and (ii) high cognitive function, low Alzheimer's disease neuropathologic change. Both experimental groups were composed of nine samples---each obtained from a separate patient---and 32,614 detected peptides. 

Ground truth differentially expressed peptides were determined by performing two-sample t-tests between experimental groups for each detected peptide. P-values were corrected for multiple hypothesis testing using the Benjamini-Hochberg procedure \cite{bj-fdr}. Peptides with corrected p-values $<$ 0.01 were considered ground truth differentially expressed.

MCAR and MNAR partitioning was performed similar to above, but this time we created three disjoint sets: training, validation and test. For the MCAR partition, 15\% of matrix entries were randomly selected without replacement for the validation set, and a separate 15\% were selected for the test set. For the MNAR partition, matrix entries corresponding to successful Bernoulli trials were assigned in an alternating fashion to either the validation or the test set. Bernoulli success probability and Gaussian distribution mean and standard deviation were tuned such that 30\% of present matrix entries were withheld from the training set.  

The validation sets were used to select the optimal hyperparameters for NMF and kNN. For MissForest a full hyperparameter search proved computationally unfeasible, so we again selected the default value of 100 for the \textit{n} trees parameter. None of the other methods had tunable hyperparameters. The following values were included in our hyperparameter searches for \textit{n} latent factors and \textit{k} neighbors: $[1,2,4,8,16,32]$.

Following hyperparameter selection, imputation was performed with each method. Differentially expressed peptides were determined for the imputed matrices as previously described. Precision-recall curves comparing ground truth to imputed differentially expressed peptides were generated with scikit-learn. For the unimputed condition, the differential expression calculation was performed as previous, while simply ignoring the missing matrix entries. That is, the differential expression test was performed on training set values only. 

\subsubsection{Quantitative peptides}

To examining the effects of imputation on the number of quantitative peptides in an experiment, we obtained data from PXD014815 \cite{matrix-matched-calib}. 
This was a serial dilution experiment in which yeast peptides were spiked into a background matrix at various known concentrations. The authors then used a custom statistical model to fit the relationship between observed and expected signal, and to determine whether increases in signal corresponded to proportional increases in peptide abundance. Peptides in which increase in signal did indeed correspond to increases in quantity across a linear range were considered quantitative.

We used this model to assess the number of quantitative peptides before and after imputation of the serial dilutions data set with various methods. MCAR partitioning was performed as described above. Hyperparameter tuning for kNN and NMF was performed as described above. The validation and test sets were imputed with each method and quantitative peptides were identified in the imputed matrices. The UpsetR package was used to generate Figure \ref{fig:rescue-experiment} \cite{UpsetR}.

\subsubsection{Lower limit of quantification}

We used the serial dilution experiment from PXD014815 to examine the effects of imputation on peptide LLOQ. We used the statistical model from Pino \textit{et al.} \cite{matrix-matched-calib} to determine the LLOQ of each detected peptide before and after imputation. One-sided binomial tests were performed to determine whether each imputation method decreases the LLOQ for significantly more peptides than it increases. Binomial p-values were corrected with the Benjamini-Hochberg procedure.

\subsection{Runtime evaluation}

We used Python's time module to compare runtimes of the various imputation methods. NMF, kNN, low value replacement, Gaussian sample and MissForest were run on 14 public proteomics data sets accessed from PRIDE. This experiment was performed on a dual CPU Intel Xeon E5-2620 machine with 32~GB RAM running CentrOS 7.6. NMF was specified to run on a maximum of 10 cores, and the remaining methods were run on a single core. This was because the kNN implementation we used, scikit-learn's KNNImputer, does not support multiprocessing, nor do our custom implementations of low value replacement and Gaussian sample impute. MissForest does support multiprocessing, though in our experience, the parallelized version of MissForest proved nearly impossible to run to completion. Thus, we choose to limit MissForest to a single core. 

%\fixme{It seems that we should run additionally run NMF with a single core for a direct comparison with the other methods.}

\section{Results}

\subsection{Evaluating with traditional criteria}

We began by assessing the performance of popular imputation methods with a traditional machine learning criterion: reconstruction error on a withheld test set. Accordingly, we obtained peptide-level quantifications for seven of the experiments shown in Table \ref{tab:data-description}. These include DIA, DDA and tandem mass tag (TMT) experiments, with a range of missingness from 0 to 92\%. We assessed the ability of the imputation methods to reconstruct missing values, after MCAR and MNAR procedures were used to simulate an additional 25\% missingness in each data set.

\begin{figure}
  \centering
  \includegraphics[width=0.95\textwidth]{figures/trad-barplots-w-labels-v4.pdf}
  \caption{{\bf Evaluating imputation methods with traditional criteria.} 
  Reconstruction error (MSE) after imputation with five methods is shown for seven public proteomics data sets. MCAR and MNAR procedures were used to simulated missing values.}
  \label{fig:traditional-eval}
\end{figure}

Our results (Figure \ref{fig:traditional-eval}) demonstrate that the best performing imputation method generally depends on the type of missingness. MissForest and NMF perform the best for all seven data sets in the MCAR condition. In the MNAR condition, the two single-value imputation methods---Gaussian sample and low value replacement---appear to work the best, though MissForest also performs well for some data sets. In both conditions the two TMT data sets, CPTAC-S047 and CPTAC-S051, yield lower reconstruction errors across all imputation methods, compared to the DDA and DIA data sets. 

\subsection{Evaluating with downstream-centric criteria}

Although the type of evaluation shown in Figure~\ref{fig:traditional-eval} is informative, we argue that reconstruction error of a held-out set is neither the most convincing nor the most relevant evaluation metric for most proteomics researchers. Additionally, good performance on reconstruction of held-out values does not guarantee good performance on certain downstream analysis tasks. Furthermore, many existing benchmarking studies make the assumption that imputation will improve performance on some downstream analysis task relative to no imputation. This assumption is generally unfounded, as imputation can introduce bias in even the best circumstances \cite{scRNAseq-false-signals, sc-impute-gene-networks}. With these considerations in mind, we compared the performance of five imputation methods on three downstream analysis tasks that we argue are more congruent with the questions proteomicists typically seek to answer.

We began with differential expression analysis. We obtained peptide-level quantifications from a DIA-based clinical study of Alzheimer's disease \cite{smtg-maccoss}. Merrihew \textit{et al.} obtained brain samples and assigned them to experimental groups based on several genetic, histopathological and cognitive criteria. We compared samples belonging to two experimental groups: (i) autosomal dominant dementia and (ii) high cognitive function, low Alzheimer's disease neuropathologic change. These experimental groups represent opposite ends of the spectrum of Alzheimer's disease severity; thus, we reasoned that they should display significant biological heterogeneity. 

\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{figures/DE-experiment-multipanel1.png}
  \caption{{\bf Evaluating the ability of imputation methods to reconstruct differentially expressed peptides.} Precision-recall curves are shown, for MCAR and MNAR simulated missingness. Data was obtained from PXD034525, a DIA study of Alzheimer's disease, and differentially expressed peptides were identified between high and low Alzheimer's associated dementia groups \cite{smtg-maccoss}. The areas under the precision-recall curves (AUCs) for MCAR are NMF: 0.59, kNN: 0.78, MissForest: 0.8, low value replacement: 0.09, Gaussian sample: 0.09, unimputed: 0.76. For MNAR, the AUCs are NMF: 0.82, kNN: 0.87, MissForest: 0.77, low value replacement: 0.53, Gaussian sample: 0.53, unimputed: 0.86.}
  \label{fig:PR-curves}
\end{figure}
 
We compared the abilities of the imputation methods to reconstruct peptides that are differentially expressed between the two experimental groups, in both MCAR and MNAR conditions (Figure~\ref{fig:PR-curves}). To perform this experiment, we identified ground truth differentially expressed peptides in the low-missingness DIA data set, simulated 30\% missingness, then imputed with various methods and identified differentially expressed peptides in the imputed matrices. We also included an unimputed condition in which differentially expressed peptides were identified directly from the unimputed training set. Thus, the sharp elbows in the MNAR precision-recall curves are due to the fact that an alpha value of 0.01 was used for determining both ground truth and imputed differentially expressed peptides. It is likely that many peptides have corrected p-values very close to the 0.01 threshold but are not considered differentially expressed, resulting in sharp decreases in precision as soon as this threshold is crossed. Nevertheless, the trends observed in Figure \ref{fig:PR-curves} remain clearly interpretable. 

In the MCAR condition, MissForest, kNN and unimputed all perform well, with areas under the curve (AUCs) of 0.80, 0.78 and 0.76, respectively. In the MNAR condition, kNN, unimputed and NMF perform the best, with respective AUCs of 0.87, 0.86 and 0.82. While the two single-value imputation methods performed well in the MNAR condition of the traditional evaluation experiment (Figure \ref{fig:traditional-eval}), they perform extremely poorly on the differential expression test, with the lowest AUCs for both MCAR and MNAR. In both conditions no imputation performs nearly the same or better than the five imputation methods. 

Next, we assessed whether imputation can generate additional quantitative peptides. While peptide detection rates have increased significantly over the past decade, not every detected peptide is necessarily quantitative. For a peptide to be considered ``quantitative,'' increases in measured signal must directly correspond to increases in peptide abundance across a linear range \cite{matrix-matched-calib}. We obtained data from a serial dilution series experiment (PXD014815) in which peptides were spiked into a background matrix at known concentration. We used a statistical model developed by Pino \textit{et al.} to determine whether each detected peptide was quantitative before and after imputation \cite{matrix-matched-calib}.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/upset-plot-v3.pdf}
  \caption{{\bf Imputation rescues quantitative peptides.} Orange indicates peptides that were quantitative in both the imputed and unimputed data sets. Blue indicates peptides that were only quantitative after imputation. Data was obtained from PXD014815 \cite{matrix-matched-calib}.}
  \label{fig:rescue-experiment}
\end{figure}

The results of this experiment (Figure \ref{fig:rescue-experiment}) show that several imputation methods produce new quantitative peptides. MissForest, kNN and NMF each produce large sets of peptides that are quantitative only after imputation (2,768 for MissForest; 1,050 for kNN; 1,128 for NMF). However, MissForest is the only method that increases the \textit{total} number of quantitative peptides relative to no imputation, producing 10,475 quantitative peptides relative to the 7,707 obtained with no imputation. 

We also assessed whether imputation can improve peptide LLOQ, which refers to the minimum abundance at which a peptide can be considered quantitative. For this analysis, we again used the serial dilution data set from Pino \textit{et al.}. We found that while imputation decreases the LLOQ for many peptides, it also increases the LLOQ for many peptides, which is the opposite of the intended effect. Strikingly, MissForest was the only method that decreased the LLOQ of significantly more peptides than it increased (Figure \ref{fig:lloq}, one-sided binomial p-value corrected with Benjamini-Hochberg $<$ 0.01).

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{figures/LLOQ-figure-multipanel-export4.png}
  \caption{{\bf Imputation affects peptide LLOQ.}  In (A) the asterisk indicates a one-sided binomial Benjamini-Hochberg corrected p-value $<$0.01. In (B--E) the LLOQs of unimputed peptides are plotted against the LLOQs of the same imputed peptides. Only peptides with changes in LLOQ following imputation are plotted. Data was obtained from PXD014815 \cite{matrix-matched-calib}.}
  \label{fig:lloq}
\end{figure}

For imputation methods to be incorporated into existing proteomics data processing workflows, they must be runnable in a reasonable time frame. We therefore assessed the runtimes of five imputation methods (Figure \ref{fig:runtime}). The two simplest methods, Gaussian sample and low value replacement, typically run in a matter of seconds; NMF and kNN run in a matter of minutes; and MissForest generally takes several hours to complete. Thus, runtime should not present a barrier for incorporation into data processing workflows, with the possible exception of MissForest with its dramatically longer runtime.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/runtimes-plotted-lineplot-cp1.png}
  \caption{{\bf Runtime comparison for imputation methods.} Each point represents a public proteomics data set. Data sets are ordered by the number of non-missing observations in their training sets after an 80\%/20\% MCAR train/test partition. This experiment was performed on a dual CPU Intel Xeon E5-2620 machine with 32~GB RAM. Ten cores were provided for NMF; the remaining methods were run on a single core.}
  \label{fig:runtime}
\end{figure} 

\subsection{Investigating variance in mass spectrometry-based proteomics data}

During the course of this study, we also investigated the extent to which the assumptions underlying several existing imputation approaches are not met by proteomics data. In particular, peptide quantifications are often assumed to be Poisson distributed, and log transformed quantifications are assumed to be Gaussian \cite{ms-dist-derivation, stat-theory-lcms, kimmel-2005}. One feature of a Poisson distribution is that variance scales linearly with the mean; one feature of a Gaussian distribution is that variance is constant across means. Parametric imputation methods with a Gaussian prior include least-squares regression, the Gaussian sample impute method, and NMF.

To investigate the empirical variance of peptide quantifications, we obtained data from four experiments, each of which contained technical replicates (Table \ref{tab:data-description}). We used three DDA experiments and one DIA. We calculated the means and variances of peptide quantifications across technical replicates, for each detected peptide, for each experiment. We found that peptide quantifications are overdispersed (Figure \ref{fig:mean-x-var}, left); that is, for nearly every peptide, the variance across replicates is greater than the mean intensity across replicates. Log transforming corrected overdispersion for some but not all peptides (Figure \ref{fig:mean-x-var}, right).

These results indicate that for two major mass spectrometry acquisition strategies, neither Poisson nor Gaussian assumptions hold. Accordingly, any parametric imputation method with a Gaussian prior is ill-suited for these data. This result also implies that differential expression analysis should be carried out with non-parametric tests such as the Wilcoxon rank sum test, instead of the parametric Student's t-test. 

We also observe that imputation with NMF and MissForest has little effect on the variance of peptide quantifications (Supplementary Figure \ref{fig:distributions-post-impute}). The Gaussian sample method, however, introduces additional variance. This finding suggests that while NMF and MissForest imputation do not profoundly effect the underlying distribution of peptide quantifications, single-value impute strategies may do so. In this way, single-value impute strategies may introduce artifacts into proteomics data when their underlying assumptions are not met.

% Noise in processed mass spectrometry quantifications is likely best described with a two-component error model \cite{ms-noise-2-component}. For low-abundance quantifications, noise is primarily Poisson, and for high-abundance quantifications, noise is primarily multiplicative [?].

\begin{figure}
  \centering
  \includegraphics[width=0.65\textwidth]{figures/mean-x-var-figure-v3.png}
  \caption{{\bf Variance of peptide quantifications is greater than expected.} Means and variances were calculated across technical replicates for every detected peptide, for four public proteomics data sets. Each dot corresponds to a peptide. The color scheme is the same in both panels. In the right panel the peptide quantifications have been log transformed.}
  \label{fig:mean-x-var}
\end{figure} 

\section{Discussion}

The two most popular imputation methods for proteomics data---Gaussian sampling and low value replacement---generally work poorly. This observation is borne out of our evaluation with both traditional and downstream-centric criteria. In the MCAR condition of the traditional evaluation experiment shown in Figure \ref{fig:traditional-eval}, both of these single-value replacement strategies consistently exhibit the worst performance. In the MNAR condition, we expected these two methods to perform well, because both methods assume that missing values are drawn entirely from the low end of the intensity spectrum, an assumption that is met by our MNAR simulation. We indeed see that in the traditional evaluation experiment, the two single-value replacement strategies perform well (Figure \ref{fig:traditional-eval}). However, in the more relevant setting of differential expression imputation, the single-value replacement strategies perform poorly under both MCAR and MNAR conditions (Figure \ref{fig:PR-curves}). Additionally, neither Gaussian sampling nor low value replacement increase the number of quantitative peptides (Figure \ref{fig:rescue-experiment}), and both increase the LLOQ for more peptides than they decrease (Figure \ref{fig:lloq}). These findings suggest that single-value replacement strategies are ill suited to common downstream tasks involving identifying differentially expressed peptides, quantifying low-abundance peptides, or quantifying peptides from small sample volumes. We thus encourage the community to move away from single-value replacement strategies.

Our study also suggests that imputation may not be necessary for differential expression analysis. 
We find that in the context of identifying ground truth differentially expressed peptides from data with simulated missingness, no imputation works roughly as well as the best imputation methods, in both MCAR and MNAR conditions (Figure \ref{fig:PR-curves}). In the MCAR condition, the largest AUC value belongs to MissForest at 0.8, only slightly higher than unimputed at 0.76. In the MNAR condition, kNN has the highest AUC at 0.87, and unimputed is close behind with 0.86. Figure \ref{fig:PR-curves} reports results for a missingness fraction of 0.25; at missingness fractions of 0.3 and 0.5, unimputed still performs about as well as the best imputation methods (Supplementary Figure \ref{fig:DE-extended}). In fact, as the missingness fraction increases, unimputed performs better and better relative to the best imputation methods.  For example, in the case of MNAR with a 50\% missingness fraction, unimputed has an AUC of 0.65, whereas the best imputation method is MissForest with an AUC of 0.55. Taken together, these results cast doubt on the practice of imputing missing values prior to differential expression analysis. This finding is in line with Wolski \textit{et al.}, which suggests that statistical models of differential expression that do not impute but rather explicitly model missingness tend to outperform traditional models \cite{prolfqua}.

We find that imputation can, however, identify new quantitative peptides (Figure \ref{fig:rescue-experiment}). The distinction between \textit{quantitative} and \textit{qualitative} peptides is important: as modern proteomic techniques increase the number of identifications, it is worth keeping in mind that not all detected peptides will necessarily be quantitative. We show that MissForest can increase the total number of quantitative peptides in an experiment (Figure \ref{fig:rescue-experiment}). Additionally, NMF and kNN can produce new subsets of quantitative peptides, even though they may still decrease the total number of quantitative peptides. Importantly, increasing the number of quantitative peptides will increase the statistical power of any downstream prediction or inference task that relies on peptide abundances. Such tasks include identifying differentially expressed peptides, clustering samples or peptides, identifying co-expression modules, and quality control. 

Imputation with MissForest can also improve the LLOQ for a subset of peptides (Figure \ref{fig:lloq}). It is worth acknowledging that while MissForest decreases the LLOQ of significantly more peptides than it increases, it does still increase the LLOQ for a large number of peptides (3,115/24,204 detected peptides). That said, any proteomics study that examines biologically important low-abundance peptides may still benefit from MissForest imputation. As the scale and sensitivity of proteomics experiments increase, MissForest---and future imputation methods---may help researchers study key peptides derived from ever-smaller sample volumes. 

Traditional machine learning evaluation criteria do not always agree with downstream-centric criteria. For example, under the MNAR condition of Figure \ref{fig:traditional-eval}, Gaussian sampling and low value replacement both perform exceptionally well, with the lowest reconstruction MSE for three of the six data sets. However, in the context of the dowstream-centric evaluation criteria of Figures \ref{fig:PR-curves}--\ref{fig:lloq}, these two methods consistently perform the worst. This seemingly contradictory finding can once again be reconciled by the fact that single-value impute strategies  explicitly assume that missing values are drawn from the low end of the distribution of peptide quantifications; this assumption is met in the MNAR condition of the traditional evaluation experiment. However, the traditional evaluation experiment is somewhat contrived and does not resemble any of the common analysis tasks in modern proteomics. We therefore argue that downstream-centric criteria are more relevant, as they more closely resemble the types of experiments proteomics researchers actually perform. We therefore urge the community to move away from machine learning-style evaluation criteria, as they can mislead with regard to the best performing imputation methods.

Finally, we provide empirical evidence that peptide quantifications exhibit more variance than can be explained under Poisson or Gaussian modeling assumptions (Figure \ref{fig:mean-x-var}). While ion detection may be a Poisson process \cite{ms-dist-derivation, stat-theory-lcms, kimmel-2005}, it is clear that the resulting peptide-level quantifications are not Poisson distributed. One property of a Poisson distribution is that the mean is equal to the variance. We find that this assumption is violated by peptide quantifications: Figure \ref{fig:mean-x-var} shows that the variance in the intensities of technical replicate peptides is greater than the corresponding means. This result holds true for both DDA and DIA experiments, and it also holds true for protein-level quantifications (Supplementary Figure \ref{fig:protein-mean-x-var}). We speculate that an unaccounted-for noise source is likely present, perhaps electrospray ionization noise. Another assumption is that log-transformed peptide quantifications are roughly Gaussian, that is, variance is constant across all intensity means. We again show that this assumption is violated in both DIA and DDA data: we observe greater-than-expected variance after log transformation.

Future imputation methods should explicitly model the variance present in proteomics data. One obvious choice of generating distribution is the negative binomial distribution, which has an additional parameter that can account for variance independent of the mean. This strategy has been employed previously to model counts from single-cell RNA sequencing experiments \cite{ZINB, neg-binom-scRNAseq}. Another option could be to perform variance stabilization prior to imputation. This is the goal with the log transformation; however, as we have shown, logging does not successfully stabilize variance. VSN, a custom variance stabilizing transformation originally developed for microarrays, has been shown to stabilize the variance of protein quantifications \cite{variance-stable-microarray, valikangas}, as has the generalized log transformation \cite{ms-noise-2-component}. However, the proteomics community is yet to broadly adopt these methods. Proteomics may also benefit from the variance stabilization technique developed by Bayat \textit{et al.}, in which a variance stabilizing function is empirically learned from the data \cite{variance-stable}. Successful modeling and variance stabilization approaches could benefit not just imputation but general data denoising procedures for quantitative proteomics.

We speculate that the unusual dimensionality of peptide-by-sample matrices, generally thousands of peptides by less than 100 samples, may cause problems for existing imputation methods, many of which were originally developed for microarray imputation and relatively square matrices. Future imputation methods may benefit from explicitly accounting for unusual dimensionality of these matrices. 

The proteomics community would benefit from easy-to-use and broadly applicable imputation methods. As previously reported \cite{Bramer:review, Webb-Robertson:review, DIMA, lazar}, we find that the best choice in imputation method depends on the analysis task and the details of the experiment. For certain tasks, existing imputation methods may not be useful at all. This suggests the need for new imputation methods that are generalizable enough to accurately handle data from any acquisition strategy and type of missingness. Deep neural networks have proven highly generalizable in other contexts. Recent ``deep'' impute methods may be a step in the right direction \cite{deep-impute}, though much work remains to be done. In the future, data-driven imputation methods will likely be broadly adopted as part of general signal processing workflows in proteomics.

\section*{Associated Content}

The code used to generate all the figures in this manuscript, as well as the supplementary material, can be found at: \url{https://github.com/Noble-Lab/2023-prot-impute-benchmark}. The custom NMF imputation model can be found at \url{https://github.com/Noble-Lab/ms_imputer}. All data sets used in this study are publicly available and can be found on PRIDE, CPTAC or Panorama \cite{PRIDE, CPTAC, panorama-public}. 

\bibliographystyle{unsrt}
\bibliography{references.bib} 

\clearpage
% Label the following figures as supplementary
\appendix
\renewcommand{\figurename}{Supplementary Figure}
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\thetable}{S\arabic{table}}
\setcounter{table}{0}
\setcounter{figure}{0}

\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{figures/partition-distributions-figure-v1.pdf}
  \caption{{\bf Distributions of partitions for both MCAR and MNAR.} Data from PXD03452 \cite{smtg-maccoss}. Log-transformed peptide-level quantifications are shown. In each case, 25\% of present matrix entries were selected for the test set.}
  \label{fig:partitions}
\end{figure} 

\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{figures/DE-testing-extended-v3.pdf}
  \caption{{\bf Additional evaluation of the ability of imputation methods to reconstruct differentially expressed peptides.} Data obtained from PXD03452, as in Figure \ref{fig:PR-curves} \cite{smtg-maccoss}. Differentially expressed peptides were identified between two experimental groups for Alzheimer's disease DIA data. AUC values are shown in parentheses.}
  \label{fig:DE-extended}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/imputed-distributions-v2.png}
  \caption{{\bf Variance of peptide quantifications is greater than expected following imputation.} Four public proteomics data sets were imputed with NMF, MissForest and Gaussian sampling. Means and variances were calculated across technical replicates for each imputed peptide, for each data set. In the righthand panels, imputed quantifications have been log-transformed.}
  \label{fig:distributions-post-impute}
\end{figure} 

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{figures/protein-distributions-fig-v0.png}
  \caption{{\bf Variance of \textit{protein} quantifications is greater than expected.} Means and variances were calculated across technical replicates for each protein, for three public protein-level quantification data sets. In the righthand panel protein quantifications have been log-transformed. Color scheme is the same in both panels.}
  \label{fig:protein-mean-x-var}
\end{figure} 

\end{document}
